# MLM pretrain BERT Model
## å°ˆæ¡ˆèªªæ˜

* ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡è³‡æ–™é›†è¨“ç·´https://huggingface.co/datasets/yentinglin/zh_TW_c4
* ä½¿ç”¨å¤šGPUä¸¦è¡Œè¨“ç·´æ–¹æ³•ã€‚
* ä½¿ç”¨Mask Language Model é è¨“ç·´BERTæ¨¡å‹ã€‚

## Datasets
* `datasets/train.txt`ï¼šè¨“ç·´ç”¨æ–‡æœ¬ï¼Œå¯æ–¼write_data.ipynbåšç”Ÿæˆã€‚
* `datasets/dev.txt`ï¼šæ¯å€‹è¨“ç·´epochçµæŸå¾Œï¼Œæœƒç”¨dev.txtåšé©—è­‰ã€‚
* `datasets/test.txt`ï¼šæ¸¬è©¦ç”¨è³‡æ–™ã€‚

## Environment
* pythonçš„ç‰ˆæœ¬ç‚º: 3.10.11
```
torch==1.13.1
transformers==4.29.2
```
* æœƒéœ€è¦ç”¨åˆ°NVIDIA/apexï¼Œå¾—å°‡apex git cloneå†å®‰è£æ‰è¡Œã€‚
```
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ python setup.py install
```
## åŸç¨‹å¼ç¢¼ç¯„ä¾‹ä¾†æºåŠDEBUG
* ä¾†æºï¼šhttps://github.com/wzzzd/pretrain_bert_with_maskLM/tree/main
* å› ç‚ºè©²é–‹æºç¢¼æˆ‘æ¸¬è©¦åªèƒ½ç”¨å–®ç‰‡GPUè¨“ç·´ï¼Œæ‰€ä»¥æ”¹å‹•äº†éƒ¨åˆ†ç¨‹å¼ã€‚
* command åŸ·è¡Œæ”¹å‹•ã€‚
```
python -m torch.distributed.launch --nproc_per_node=4 --master_port='29301' --use_env main.py
```
* åˆå§‹åŒ–GPU processè¨­ç½®æ”¹å‹•ï¼Œè§£æ±ºå¡é “çš„å•é¡Œã€‚
```
torch.distributed.init_process_group(backend="nccl")
```
* å¤šprocessåŸ·è¡Œæ™‚å°‡å„å€‹local_rankè™Ÿç¢¼å¯«å…¥modelï¼Œè§£æ±ºæ‰€æœ‰processåªå°‡modelè¼‰å…¥åˆ°GPU 0å•é¡Œã€‚
```
model = torch.nn.parallel.DistributedDataParallel(model,
                                                  device_ids=[local_rank],
                                                  output_device=local_rank)
```

## Get Start

### å–®å¡æ¨¡å¼(æ¸¬è©¦)

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.path_model_predict`ã€‚

é¸æ“‡æƒ³è¦çš„ç¬¬nå€‹epochè¨“ç·´çš„modelå†é‹è¡Œã€‚
* å¦‚è¦ç¬¬9å€‹ï¼Œå‰‡è¼¸å…¥`self.path_model_predict = os.path.join(self.path_model_save, 'epoch_9')`
```
python main.py test
```

### å¤šå¡æ¨¡å¼ï¼ˆè¨“ç·´ï¼‰
å¦‚æœä½ è¶³å¤ å¹¸é‹ï¼Œæ“æœ‰äº†å¤šå¼µGPUå¡ï¼Œé‚£éº¼æ­å–œä½ ï¼Œä½ å¯ä»¥é€²å…¥èµ·é£›æ¨¡å¼ã€‚ğŸš€ğŸš€

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.num_epochs, self.batch_size, self.sen_max_length`ï¼Œå†é‹è¡Œã€‚

* å¦‚è¦è¨­ç½®è¨“ç·´10å€‹epochï¼Œå‰‡è¼¸å…¥`self.num_epochs = 10`
* å¦‚è¦è¨­ç½® BERTæœ€é•·é•·åº¦(<=512)ï¼Œå‰‡è¼¸å…¥`self.sen_max_length = 512`
* å¦‚è¦è¨­ç½® batch_sizeå¤§å°(ä¾ç…§å¯å®¹ç´sizeè¨­ç½®)ï¼Œè¨­ç½®16å‰‡è¼¸å…¥`self.batch_size = 16`
```
python -m torch.distributed.launch --nproc_per_node=4 --master_port='29301' --use_env main.py train
```

* ä½¿ç”¨torchçš„`nn.parallel.DistributedDataParallel`æ¨¡å¡Šé€²è¡Œå¤šå¡è¨“ç·´ã€‚
* <font color=#009393>`master_port`ï¼šmasterç¯€é»çš„portè™Ÿï¼Œåœ¨ä¸åŒçš„ç¯€é»ä¸Šmaster_addrå’Œmaster_portçš„è¨­ç½®æ˜¯ä¸€æ¨£çš„ï¼Œç”¨ä¾†é€²è¡Œé€šä¿¡ï¼Œportæˆ‘è¨­ç½®'29301'ã€‚</font>
* <font color=#009393>`nproc_per_node`ï¼šä¸€å€‹ç¯€é»ä¸­é¡¯å¡çš„æ•¸é‡ï¼Œæˆ‘æœ‰4ç‰‡GPUï¼Œæ‰€ä»¥è¨­ç½®4ã€‚ </font>

## training
ä½¿ç”¨äº¤å‰ç†µï¼ˆcross-entropyï¼‰ä½œç‚ºæå¤±å‡½æ•¸ï¼Œå›°æƒ‘åº¦ï¼ˆperplexityï¼‰å’ŒLossä½œç‚ºè©•åƒ¹æŒ‡æ¨™ä¾†é€²è¡Œè¨“ç·´ã€‚

## test
çµæœä¿å­˜åœ¨`dataset/output/pred_data.csv`ï¼Œåˆ†åˆ¥åŒ…å«ä¸‰åˆ—ï¼š
- `src`è¡¨ç¤ºåŸå§‹è¼¸å…¥
- `pred`è¡¨ç¤ºæ¨¡å‹é æ¸¬
- `mask`è¡¨ç¤ºæ¨¡å‹è¼¸å…¥ï¼ˆå¸¶æœ‰maskå’Œpadç­‰tokenï¼‰

## ç¯„ä¾‹

```
src:  [CLS] art education and first professional work [SEP]
pred: [CLS] art education and first class work [SEP]
mask: [CLS] art education and first [MASK] work [SEP] [PAD] [PAD] [PAD] ...
```


# Reference

ã€Bertã€‘[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)

ã€transformersã€‘[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

ã€datasetsã€‘[https://huggingface.co/datasets/yentinglin/zh_TW_c4](https://huggingface.co/datasets/yentinglin/zh_TW_c4)




