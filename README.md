# MLM pretrain BERT Model
## å°ˆæ¡ˆèªªæ˜

* ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡è³‡æ–™é›†è¨“ç·´https://huggingface.co/datasets/yentinglin/zh_TW_c4
* å¤šç‰‡GPUä¸¦è¡Œè¨“ç·´ã€‚
* Mask Language Model é è¨“ç·´BERTæ¨¡å‹ã€‚

## Datasets
* `datasets/train.txt`ï¼šè¨“ç·´ç”¨æ–‡æœ¬ï¼Œå¯æ–¼write_data.ipynbåšç”Ÿæˆã€‚
* `datasets/dev.txt`ï¼šæ¯å€‹è¨“ç·´epochçµæŸå¾Œï¼Œæœƒç”¨dev.txtåšé©—è­‰ã€‚
* `datasets/test.txt`ï¼šæ¸¬è©¦ç”¨è³‡æ–™ã€‚

## Environment
* Ubuntu20.04
* CUDA Version: 11.7
* GeForce RTX 3090 * 4
* pythonçš„ç‰ˆæœ¬ç‚º: 3.10.11
```
torch==1.13.1
transformers==4.29.2
```
* æœƒéœ€è¦ç”¨åˆ°NVIDIA/apexï¼Œå¾—å°‡apex git cloneå†å®‰è£æ‰è¡Œã€‚
```
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ python setup.py install
```
## ç¨‹å¼ç¢¼ç¯„ä¾‹ä¾†æºDEBUG
* å› ç‚ºè©²é–‹æºç¢¼æˆ‘æ¸¬è©¦åªèƒ½ç”¨å–®ç‰‡GPUè¨“ç·´ï¼Œæ‰€ä»¥æ”¹å‹•äº†éƒ¨åˆ†ç¨‹å¼ã€‚
* command åŸ·è¡Œæ”¹å‹•ã€‚
```
python -m torch.distributed.launch --nproc_per_node=4 --master_port='29301' --use_env main.py
```
* åˆå§‹åŒ–GPU processè¨­ç½®æ”¹å‹•ï¼Œè§£æ±ºå¡é “çš„å•é¡Œã€‚
```
torch.distributed.init_process_group(backend="nccl")
```
* å¤šprocessåŸ·è¡Œæ™‚å°‡å„å€‹local_rankè™Ÿç¢¼å¯«å…¥modelï¼Œè§£æ±ºæ‰€æœ‰processåªå°‡modelè¼‰å…¥åˆ°GPU 0å•é¡Œã€‚
```
model = torch.nn.parallel.DistributedDataParallel(model,
                                                  device_ids=[local_rank],
                                                  output_device=local_rank)
```

## Get Start

### å–®å¡æ¨¡å¼(æ¸¬è©¦)

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.path_model_predict`ã€‚

é¸æ“‡æƒ³è¦çš„ç¬¬nå€‹epochè¨“ç·´çš„modelå†é‹è¡Œã€‚
* å¦‚è¦è¨“ç·´åˆ°ç¬¬9å€‹epochçš„Modelåƒæ•¸ï¼Œå‰‡è¼¸å…¥`self.path_model_predict = os.path.join(self.path_model_save, 'epoch_9')`
```
python main.py test
```

### å¤šå¡æ¨¡å¼ï¼ˆè¨“ç·´ï¼‰
å¦‚æœä½ è¶³å¤ å¹¸é‹ï¼Œæ“æœ‰äº†å¤šå¼µGPUå¡ï¼Œé‚£éº¼æ­å–œä½ ï¼Œä½ å¯ä»¥é€²å…¥èµ·é£›æ¨¡å¼ã€‚ğŸš€ğŸš€

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„ `self.num_epochs, self.batch_size, self.sen_max_length` ï¼Œå†é‹è¡Œã€‚

* è¨­ç½®è¨“ç·´10å€‹epochï¼Œå‰‡è¼¸å…¥`self.num_epochs = 10`
* è¨­ç½® BERTé•·åº¦(<=512)ï¼Œå¦‚è¨­ç½®æœ€é•·å‰‡è¼¸å…¥`self.sen_max_length = 512`
* è¨­ç½® batch_sizeå¤§å°(ä¾ç…§å¯å®¹ç´sizeè¨­ç½®)ï¼Œå¦‚è¨­ç½®16å‰‡è¼¸å…¥`self.batch_size = 16`
```
python -m torch.distributed.launch --nproc_per_node=4 --master_port='29301' --use_env main.py train
```

* ä½¿ç”¨torchçš„`nn.parallel.DistributedDataParallel`æ¨¡å¡Šé€²è¡Œå¤šå¡è¨“ç·´ã€‚
* <font color=#009393>`master_port`ï¼šmasterç¯€é»çš„portè™Ÿï¼Œåœ¨ä¸åŒçš„ç¯€é»ä¸Šmaster_addrå’Œmaster_portçš„è¨­ç½®æ˜¯ä¸€æ¨£çš„ï¼Œç”¨ä¾†é€²è¡Œé€šä¿¡ï¼Œportæˆ‘è¨­ç½®'29301'ã€‚</font>
* <font color=#009393>`nproc_per_node`ï¼šä¸€å€‹ç¯€é»ä¸­é¡¯å¡çš„æ•¸é‡ï¼Œæˆ‘æœ‰4ç‰‡GPUï¼Œæ‰€ä»¥è¨­ç½®4ã€‚ </font>

### è¶…å¤§è³‡æ–™é‡è®€å–(è¨“ç·´) [2023/08/17æ”¹å‹•]
å¦‚æœæœ‰è³‡æ–™é‡å¤§åˆ°CPU RAMç„¡æ³•è®€å–çš„æƒ…æ³ï¼Œè«‹å…ˆå°‡æª”æ¡ˆåˆ†å‰²å¯«å…¥åˆ°è·¯å¾‘`./datasets/train_shard`

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.huge_data_file_data_length`ï¼Œæ¯å€‹æª”æ¡ˆçš„è³‡æ–™æœ‰å¤šå°‘ç­†ï¼Œå‰‡è¼¸å…¥å¤šå°‘ã€‚æˆ‘åˆ†å‰²æˆæ¯å€‹æª”æ¡ˆ160000ç­†ï¼Œå‰‡è¼¸å…¥160000ã€‚
* æœ¬äººä½¿ç”¨çš„è³‡æ–™é‡æœ‰ç™¾è¬ä»¥ä¸Šï¼Œhttps://huggingface.co/datasets/yentinglin/zh_TW_c4 çš„è³‡æ–™ç¸½å…±æœ‰500è¬ç­†æ•¸ã€5 Billoinçš„tokensã€‚
* ä½¿ç”¨`torch.distributed.launch`åŸ·è¡Œæœ‰å€‹å„ªé»åŠç¼ºé»ã€‚
- å„ªé»ï¼šå¤šprocesså¯ä»¥å¿«é€Ÿå°‡DistributedSampler(tokenized_datasets)å®Œæˆ
- ç¼ºé»ï¼šæœƒå¤šæ¬¡è®€å–ç›¸åŒæª”æ¡ˆå†æ•´ç†é€²DistributedSampler()ã€‚
* å‡è¨­æœ‰4ç‰‡GPUï¼Œ`world_size=4`ï¼Œæœƒå°è‡´ç›¸åŒè³‡æ–™æª”æ¡ˆåŒæ™‚è¢«é‡è¤‡è®€å–å››æ¬¡ã€‚æœ¬äººCPU RAM æœ‰128GBä¾ç„¶ä¸å¤ ã€‚
```
python -m torch.distributed.launch --nproc_per_node=4 --master_port='29301' --use_env main.py huge_train
```

* åœ¨`Trainer.py`æ–°å¢`def huge_data_train()`ï¼Œå¯ä»¥çœ‹å‡ºåœ¨è¨“ç·´éç¨‹æœƒè®€å–ä¸‹å€‹æª”æ¡ˆå†è½‰æˆæ–°çš„DataLoaderå½¢å¼ï¼Œä¹‹å‰æ˜¯ç›´æ¥æ‰€æœ‰è¦è¨“ç·´è³‡æ–™è½‰æˆDataLoaderã€‚
```
# Trainer.py
def huge_data_train(self,local_rank,world_size):
    ...
    for epoch in range(self.config.num_epochs):
      for shard in file_list:
        file_name = '/train_shard/'+shard
        train_loader = dm.data_process(file_name, self.tokenizer)
    ...
```
* æ–¼Trainingéç¨‹åè¦†è®€å–æ–°æª”æ¡ˆï¼Œå†å‰µå»ºæ–°çš„DataLoaderæœƒæœ‰å€‹å•é¡Œã€‚
* å„ªåŒ–å™¨çš„ `learning rate scheduler` åƒæ•¸ `training steps` å¾—é‡æ–°ç²¾ç®—ã€‚
* æ‰€ä»¥å¾—æ–¼`Config.py`æ–‡ä»¶ä¸­çš„ `self.huge_data_file_data_length` è¼¸å…¥æ¯å€‹æª”æ¡ˆçš„è³‡æ–™æœ‰å¤šå°‘ç­†ã€‚
```
num_training_steps = int(self.config.num_epochs * self.config.huge_data_file_data_length * len(file_list) / (self.config.batch_size*world_size))
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=self.config.num_warmup_steps,
    num_training_steps=num_training_steps
)
```
## training
ä½¿ç”¨äº¤å‰ç†µï¼ˆcross-entropyï¼‰ä½œç‚ºæå¤±å‡½æ•¸ï¼Œå›°æƒ‘åº¦ï¼ˆperplexityï¼‰å’ŒLossä½œç‚ºè©•åƒ¹æŒ‡æ¨™ä¾†é€²è¡Œè¨“ç·´ã€‚

## test
çµæœä¿å­˜åœ¨`dataset/output/pred_data.csv`ï¼Œåˆ†åˆ¥åŒ…å«ä¸‰åˆ—ï¼š
- `src`è¡¨ç¤ºåŸå§‹è¼¸å…¥
- `pred`è¡¨ç¤ºæ¨¡å‹é æ¸¬
- `mask`è¡¨ç¤ºæ¨¡å‹è¼¸å…¥ï¼ˆå¸¶æœ‰maskå’Œpadç­‰tokenï¼‰

## ç¯„ä¾‹

```
src:  [CLS] art education and first professional work [SEP]
pred: [CLS] art education and first class work [SEP]
mask: [CLS] art education and first [MASK] work [SEP] [PAD] [PAD] [PAD] ...
```


# Reference

ã€Bertã€‘[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)

ã€transformersã€‘[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

ã€datasetsã€‘[https://huggingface.co/datasets/yentinglin/zh_TW_c4](https://huggingface.co/datasets/yentinglin/zh_TW_c4)




