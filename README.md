# Pretraining BERT Model
## å°ˆæ¡ˆèªªæ˜

* ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡è³‡æ–™é›†è¨“ç·´https://huggingface.co/datasets/yentinglin/zh_TW_c4
* ä½¿ç”¨å¤šGPUä¸¦è¡Œè¨“ç·´æ–¹æ³•
* ä½¿ç”¨Mask Language Model é è¨“ç·´BERTæ¨¡å‹

## Datasets
* `datasets/train.txt`ï¼šè¨“ç·´ç”¨æ–‡æœ¬ï¼Œå¯æ–¼write_data.ipynbåšç”Ÿæˆã€‚
* `datasets/dev.txt`ï¼šæ¯å€‹è¨“ç·´epochçµæŸå¾Œï¼Œæœƒç”¨dev.txtåšé©—è­‰ã€‚
* `datasets/test.txt`ï¼šæ¸¬è©¦ç”¨è³‡æ–™ã€‚

## Environment
* pythonçš„ç‰ˆæœ¬ç‚º: 3.10.11
```
torch==1.13.1
transformers==4.29.2
```
* æœƒéœ€è¦ç”¨åˆ°NVIDIA/apexï¼Œå¾—å°‡apex git cloneå†å®‰è£æ‰è¡Œã€‚
```
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ python setup.py install
```
## Get Start

### å–®å¡æ¨¡å¼(æ¸¬è©¦)

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.mode='test'`ï¼Œå†é‹è¡Œ
```
python main.py
```

### å¤šå¡æ¨¡å¼ï¼ˆè¨“ç·´ï¼‰
å¦‚æœä½ è¶³å¤ å¹¸é‹ï¼Œæ“æœ‰äº†å¤šå¼µGPUå¡ï¼Œé‚£éº¼æ­å–œä½ ï¼Œä½ å¯ä»¥é€²å…¥èµ·é£›æ¨¡å¼ã€‚ğŸš€ğŸš€

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.mode='train'`ï¼Œå†é‹è¡Œ
```
python -m torch.distributed.launch --nproc_per_node=4 --master_port='29301' --use_env main.py
```

** ä½¿ç”¨torchçš„`nn.parallel.DistributedDataParallel`æ¨¡å¡Šé€²è¡Œå¤šå¡è¨“ç·´ã€‚

* <font color=#009393>`master_port`ï¼šmasterç¯€é»çš„portè™Ÿï¼Œåœ¨ä¸åŒçš„ç¯€é»ä¸Šmaster_addrå’Œmaster_portçš„è¨­ç½®æ˜¯ä¸€æ¨£çš„ï¼Œç”¨ä¾†é€²è¡Œé€šä¿¡ï¼Œportæˆ‘è¨­ç½®'29301'ã€‚</font>
* <font color=#009393>`nproc_per_node`ï¼šä¸€å€‹ç¯€é»ä¸­é¡¯å¡çš„æ•¸é‡ï¼Œæˆ‘æœ‰4ç‰‡GPUï¼Œæ‰€ä»¥è¨­ç½®4ã€‚ </font>

## training
ä½¿ç”¨äº¤å‰ç†µï¼ˆcross-entropyï¼‰ä½œç‚ºæå¤±å‡½æ•¸ï¼Œå›°æƒ‘åº¦ï¼ˆperplexityï¼‰å’ŒLossä½œç‚ºè©•åƒ¹æŒ‡æ¨™ä¾†é€²è¡Œè¨“ç·´ï¼Œè¨“ç·´éç¨‹å¦‚ä¸‹ï¼š
<!-- ![](./picture/experiment.png) -->
<img src=./picture/experiment.png width=70% />

## test
çµæœä¿å­˜åœ¨`dataset/output/pred_data.csv`ï¼Œåˆ†åˆ¥åŒ…å«ä¸‰åˆ—ï¼š
- `src`è¡¨ç¤ºåŸå§‹è¼¸å…¥
- `pred`è¡¨ç¤ºæ¨¡å‹é æ¸¬
- `mask`è¡¨ç¤ºæ¨¡å‹è¼¸å…¥ï¼ˆå¸¶æœ‰maskå’Œpadç­‰tokenï¼‰

## ç¯„ä¾‹

```
src:  [CLS] art education and first professional work [SEP]
pred: [CLS] art education and first class work [SEP]
mask: [CLS] art education and first [MASK] work [SEP] [PAD] [PAD] [PAD] ...
```


# Reference

ã€Bertã€‘[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)

ã€transformersã€‘[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

ã€datasetsã€‘[https://huggingface.co/docs/datasets/quicktour.html](https://huggingface.co/docs/datasets/quicktour.html)




